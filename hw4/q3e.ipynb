{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Load and combine the data\n",
    "# # The data has two sources e=1 (observational) and e=2 (experimental)\n",
    "# q3 = pd.read_csv(\"data_for_HW4.csv\")\n",
    "\n",
    "# # We will construct a single model that uses both datasets.\n",
    "# # Let's use a flexible parametric specification that allows for \n",
    "# # treatment effect heterogeneity via interactions between T and X.\n",
    "# #\n",
    "# # Model:\n",
    "# #   Y = α + Xβ + T(γ0 + Xγ) + ε\n",
    "# #\n",
    "# # This model can be written as:\n",
    "# #   Y = α + Xβ + Tγ0 + (T * X)γ + ε\n",
    "# #\n",
    "# # This is a linear model in parameters. The treatment effect at point X is:\n",
    "# #   τ(X) = γ0 + Xγ\n",
    "# #\n",
    "# # By fitting this model on the combined data (e=1 and e=2), we share all \n",
    "# # parameters and assume a common structural relationship, using both datasets \n",
    "# # simultaneously.\n",
    "# #\n",
    "# # Note: This code assumes that treatment T is binary {0,1}. The model includes \n",
    "# # all main effects of X and their interactions with T, allowing the treatment \n",
    "# # effect to vary with X. Since e=2 is randomized, it identifies the parameters. \n",
    "# # Using e=1 as well can increase precision. We are not separating the datasets, \n",
    "# # but fitting one unified model.\n",
    "\n",
    "# # Extract variables\n",
    "# Y = q3['y'].values\n",
    "# T = q3['t'].values\n",
    "# X = q3[['x.1','x.2','x.3','x.4','x.5']].values\n",
    "\n",
    "# # Create interaction terms between T and X\n",
    "# # We'll form a design matrix: [1, X, T, T*X]\n",
    "# n = X.shape[0]\n",
    "# intercept = np.ones((n,1))\n",
    "# TX = (T.reshape(-1,1) * X)  # Element-wise multiplication\n",
    "\n",
    "# # Final design matrix:\n",
    "# # Z = [1, X, T, T*X]\n",
    "# Z = np.hstack([intercept, X, T.reshape(-1,1), TX])\n",
    "\n",
    "# # Fit the linear model using OLS\n",
    "# model = sm.OLS(Y, Z).fit()\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # Interpretation:\n",
    "# # The fitted model gives estimates of α, β, γ0, and γ.\n",
    "# # The treatment effect function at X is: τ(X) = γ0 + Xγ.\n",
    "# # After fitting the model, we have a single set of parameters learned \n",
    "# # from both datasets together. This leverages the assumption that both \n",
    "# # datasets come from the same structural equation. The RCT data (e=2) \n",
    "# # secures identification, while the observational data (e=1) can provide \n",
    "# # additional precision.\n",
    "\n",
    "# # You can extract the parameters and use them to calculate τ(X) at any point X.\n",
    "# params = model.params\n",
    "\n",
    "# # Parameter mapping:\n",
    "# # params = [α, β_1, β_2, β_3, β_4, β_5, γ0, γ_1, γ_2, γ_3, γ_4, γ_5]\n",
    "# alpha = params[0]\n",
    "# beta = params[1:6]    # Coefficients for X\n",
    "# gamma0 = params[6]    \n",
    "# gamma = params[7:12]  # Coefficients for (T*X)\n",
    "\n",
    "# # For a given X (row vector of length 5), tau(X) = γ0 + X * γ\n",
    "# def tau(x_vec):\n",
    "#     return gamma0 + np.dot(x_vec, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       coef   std err          t  P>|t|     2.5 %    97.5 %\n",
      "t  1.314469  0.020126  65.310582    0.0  1.275022  1.353917\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import doubleml as dml\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the combined dataset\n",
    "df = pd.read_csv(\"data_for_HW4.csv\")\n",
    "\n",
    "df.rename(columns=lambda c: c.replace(\".\", \"_\"), inplace=True)\n",
    "\n",
    "# Specify outcome, treatment, and covariates\n",
    "y_col = 'y'\n",
    "d_col = 't'\n",
    "x_cols = ['x_1', 'x_2', 'x_3', 'x_4', 'x_5']\n",
    "\n",
    "for col in x_cols:\n",
    "    df[col + \"_e\"] = df[col] * (df['e'] - 1)\n",
    "\n",
    "x_cols = [col for col in list(df.columns) if col not in [y_col, d_col]]\n",
    "\n",
    "# Create a DoubleMLData object\n",
    "obj_dml_data = dml.DoubleMLData(df, y_col, d_col, x_cols)\n",
    "\n",
    "# Set up learners for the nuisance functions\n",
    "# ml_g: model for E[Y|X], ml_m: model for P(T=1|X)\n",
    "ml_g = RandomForestRegressor(max_features='sqrt', random_state=42)\n",
    "ml_m = RandomForestClassifier(max_features='sqrt', random_state=42)\n",
    "\n",
    "ml_g = GridSearchCV(\n",
    "    estimator=ml_g,\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 200, 500],\n",
    "        'max_features': ['sqrt'],\n",
    "        'max_depth': [1, 3, 5]\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ml_m = GridSearchCV(\n",
    "    estimator=ml_m,\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 200, 500],\n",
    "        'max_features': ['sqrt'],\n",
    "        'max_depth': [1, 3, 5],\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Initialize the DoubleMLPLR object\n",
    "dml_plr_rf = dml.DoubleMLPLR(obj_dml_data, ml_g, ml_m, score='partialling out')\n",
    "\n",
    "# Fit the model\n",
    "dml_plr_rf.fit()\n",
    "\n",
    "# Print the summary of the estimated causal effect\n",
    "print(dml_plr_rf.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     coef   std err          t  P>|t|     2.5 %    97.5 %\n",
      "t  1.5903  0.018785  84.658239    0.0  1.553482  1.627117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/doubleml/utils/_checks.py:204: UserWarning: Propensity predictions from learner GridSearchCV(cv=5, estimator=MLPClassifier(max_iter=3000, random_state=42),\n",
      "             n_jobs=-1,\n",
      "             param_grid={'activation': ['relu'], 'alpha': [0.01, 0.1, 0.5],\n",
      "                         'hidden_layer_sizes': [(32, 32), (32, 32, 32, 32),\n",
      "                                                (64, 32, 16)],\n",
      "                         'learning_rate': ['adaptive'], 'solver': ['adam']}) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(f'Propensity predictions from learner {str(learner)} for'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import doubleml as dml\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the combined dataset\n",
    "df = pd.read_csv(\"data_for_HW4.csv\")\n",
    "\n",
    "df.rename(columns=lambda c: c.replace(\".\", \"_\"), inplace=True)\n",
    "\n",
    "# Specify outcome, treatment, and covariates\n",
    "y_col = 'y'\n",
    "d_col = 't'\n",
    "x_cols = ['x_1', 'x_2', 'x_3', 'x_4', 'x_5']\n",
    "\n",
    "for col in x_cols:\n",
    "    df[col + \"_e\"] = df[col] * (df['e'] - 1)\n",
    "\n",
    "x_cols = [col for col in list(df.columns) if col not in [y_col, d_col]]\n",
    "\n",
    "# Create a DoubleMLData object\n",
    "obj_dml_data = dml.DoubleMLData(df, y_col, d_col, x_cols)\n",
    "\n",
    "# Set up learners for the nuisance functions\n",
    "# ml_g: model for E[Y|X], ml_m: model for P(T=1|X)\n",
    "ml_g = MLPRegressor(random_state=42, max_iter=3000)\n",
    "ml_m = MLPClassifier(random_state=42, max_iter=3000)\n",
    "\n",
    "ml_g = GridSearchCV(\n",
    "    estimator=ml_g,\n",
    "    param_grid={\n",
    "        'hidden_layer_sizes': [(32, 32), (32, 32, 32, 32,), (64, 32, 16)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.01, .1, .5],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "ml_m = GridSearchCV(\n",
    "    estimator=ml_m,\n",
    "    param_grid={\n",
    "        'hidden_layer_sizes': [(32, 32), (32, 32, 32, 32,), (64, 32, 16)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [0.01, .1, .5],\n",
    "        'learning_rate': ['adaptive']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Initialize the DoubleMLPLR object\n",
    "dml_plr_dnn = dml.DoubleMLPLR(obj_dml_data, ml_g, ml_m, score='partialling out')\n",
    "\n",
    "# Fit the model\n",
    "dml_plr_dnn.fit()\n",
    "\n",
    "# Print the summary of the estimated causal effect\n",
    "print(dml_plr_dnn.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
