{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression         False  0.874867   \n",
      "1        LogisticRegression        LinearRegression          True  0.871097   \n",
      "2        LogisticRegression   RandomForestRegressor         False  0.874867   \n",
      "3        LogisticRegression   RandomForestRegressor          True  0.871097   \n",
      "4        LogisticRegression  NeuralNetworkRegressor         False  0.874867   \n",
      "5        LogisticRegression  NeuralNetworkRegressor          True  0.871097   \n",
      "6        LogisticRegression         LassoRegression         False  0.874867   \n",
      "7        LogisticRegression         LassoRegression          True  0.871097   \n",
      "8    RandomForestClassifier        LinearRegression         False  0.958338   \n",
      "9    RandomForestClassifier        LinearRegression          True       NaN   \n",
      "10   RandomForestClassifier   RandomForestRegressor         False  0.958338   \n",
      "11   RandomForestClassifier   RandomForestRegressor          True       NaN   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.958338   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor          True       NaN   \n",
      "14   RandomForestClassifier         LassoRegression         False  0.958338   \n",
      "15   RandomForestClassifier         LassoRegression          True       NaN   \n",
      "16  NeuralNetworkClassifier        LinearRegression         False  1.067900   \n",
      "17  NeuralNetworkClassifier        LinearRegression          True  0.920863   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.067900   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor          True  0.920863   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.067900   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  0.920863   \n",
      "22  NeuralNetworkClassifier         LassoRegression         False  1.067900   \n",
      "23  NeuralNetworkClassifier         LassoRegression          True  0.920863   \n",
      "\n",
      "      Tau_DR  \n",
      "0  -1.995883  \n",
      "1  -2.004188  \n",
      "2  -1.951904  \n",
      "3  -1.958615  \n",
      "4  -2.220714  \n",
      "5  -2.090103  \n",
      "6  -2.036195  \n",
      "7  -2.042983  \n",
      "8  -1.843984  \n",
      "9        NaN  \n",
      "10 -1.800005  \n",
      "11       NaN  \n",
      "12 -2.068815  \n",
      "13       NaN  \n",
      "14 -1.884296  \n",
      "15       NaN  \n",
      "16 -1.644606  \n",
      "17 -1.913626  \n",
      "18 -1.600627  \n",
      "19 -1.868053  \n",
      "20 -1.869437  \n",
      "21 -1.999540  \n",
      "22 -1.684918  \n",
      "23 -1.952421  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and preprocess data\n",
    "#-------------------------------------------\n",
    "census = pd.read_csv(\"census2000.csv\")\n",
    "census.rename(columns=lambda c: c[1:] if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "# Treatment indicator: T = 1 if male, 0 if female\n",
    "census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "# Outcome: log wage = log(income/hours)\n",
    "census['wage'] = census['income'] / census['hours']\n",
    "census['Y'] = np.log(census['wage'])\n",
    "\n",
    "# Define features (excluding income, hours, sex, wage, Y)\n",
    "# Here we use age, marital, race, education as features\n",
    "# Convert categorical variables to dummies\n",
    "X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "Y = census['Y'].values\n",
    "T = census['T'].values\n",
    "\n",
    "# Proportion treated\n",
    "p_hat = T.mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "# Functions for first-step estimation\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit propensity score model.\n",
    "    model: classifier with predict_proba\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.fit(X, T)\n",
    "    p_hat_x = model.predict_proba(X)[:,1]\n",
    "    # truncate probability estimates\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    return model, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit outcome regression model for the control group only.\n",
    "    model: regressor with predict\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    # Fit only on controls\n",
    "    model.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = model.predict(X)\n",
    "    return model, mu0_hat_x\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimators\n",
    "#-------------------------------------------\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with mu1_hat = (1/n)*sum(T_i * Y_i / p_hat)\n",
    "    and mu0_hat = (1/n)*sum{ (1-T_i)*p_hat(X_i)*Y_i / [ (1 - p_hat(X_i)) ] }\n",
    "\n",
    "    p_hat is the average of T (overall proportion treated).\n",
    "    p_hat_x is the estimated propensity for each unit.\n",
    "    mu0_hat_x is the estimated outcome regression for the control condition.\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Doubly robust estimator of ATT:\n",
    "    tau_DR = (1/n)*sum( t_i * y_i / p_hat ) \n",
    "             - (1/p_hat)* (1/n)* sum( t_i * mu0_hat(x_i) + (1-t_i)*p_hat(x_i)*Y_i/(1 - p_hat(x_i)) )\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    term1 = np.mean(T * Y / p_hat)\n",
    "    term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
    "    tau_DR = term1 - term2\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting option\n",
    "#-------------------------------------------\n",
    "\n",
    "def cross_fit_estimates(X, Y, T, p_hat, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Perform cross-fitting: split data into folds, \n",
    "    estimate nuisance functions on one fold and evaluate on the other.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Fit models on training fold\n",
    "        prop_model_fold, _ = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, _ = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    # Compute estimators\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "def single_fit_estimates(X, Y, T, p_hat, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    No cross-fitting: fit on entire sample.\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Model configurations\n",
    "#-------------------------------------------\n",
    "# We consider various methods for p_hat(x) and mu0_hat(x):\n",
    "# For demonstration: logistic regression, random forest, neural net, lasso, parametric linear.\n",
    "\n",
    "propensity_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "outcome_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=500, random_state=42),\n",
    "    \"LassoRegression\": Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Convert X to numpy for convenience\n",
    "X_np = X.values\n",
    "\n",
    "#-------------------------------------------\n",
    "# Run the analyses and show results\n",
    "#-------------------------------------------\n",
    "results = []\n",
    "\n",
    "for p_name, p_model in propensity_models.items():\n",
    "    for m_name, m_model in outcome_models.items():\n",
    "        # Without cross-fitting\n",
    "        tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_hat, p_model, m_model)\n",
    "\n",
    "        # With cross-fitting\n",
    "        tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_hat, p_model, m_model, n_splits=2)\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": False,\n",
    "            \"Tau_PI\": tau_PI_no_cf,\n",
    "            \"Tau_DR\": tau_DR_no_cf\n",
    "        })\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": True,\n",
    "            \"Tau_PI\": tau_PI_cf,\n",
    "            \"Tau_DR\": tau_DR_cf\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# This results_df now contains the ATT estimates for both the PI and DR estimators,\n",
    "# with and without cross-fitting, and for different first-step models.\n",
    "# You can further analyze, plot, or discuss these results as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"q2c-data/preliminary_2_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propensity_Model</th>\n",
       "      <th>Outcome_Model</th>\n",
       "      <th>CrossFitting</th>\n",
       "      <th>Tau_PI</th>\n",
       "      <th>Tau_DR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-1.995883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.004188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-1.951904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-1.958615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-2.220714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.090103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-2.036195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.042983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.843984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.800005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-2.068815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.884296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.644606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.913626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.600627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.868053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.869437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.999540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.684918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.952421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
       "0        LogisticRegression        LinearRegression         False  0.874867   \n",
       "1        LogisticRegression        LinearRegression          True  0.871097   \n",
       "2        LogisticRegression   RandomForestRegressor         False  0.874867   \n",
       "3        LogisticRegression   RandomForestRegressor          True  0.871097   \n",
       "4        LogisticRegression  NeuralNetworkRegressor         False  0.874867   \n",
       "5        LogisticRegression  NeuralNetworkRegressor          True  0.871097   \n",
       "6        LogisticRegression         LassoRegression         False  0.874867   \n",
       "7        LogisticRegression         LassoRegression          True  0.871097   \n",
       "8    RandomForestClassifier        LinearRegression         False  0.958338   \n",
       "9    RandomForestClassifier        LinearRegression          True       NaN   \n",
       "10   RandomForestClassifier   RandomForestRegressor         False  0.958338   \n",
       "11   RandomForestClassifier   RandomForestRegressor          True       NaN   \n",
       "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.958338   \n",
       "13   RandomForestClassifier  NeuralNetworkRegressor          True       NaN   \n",
       "14   RandomForestClassifier         LassoRegression         False  0.958338   \n",
       "15   RandomForestClassifier         LassoRegression          True       NaN   \n",
       "16  NeuralNetworkClassifier        LinearRegression         False  1.067900   \n",
       "17  NeuralNetworkClassifier        LinearRegression          True  0.920863   \n",
       "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.067900   \n",
       "19  NeuralNetworkClassifier   RandomForestRegressor          True  0.920863   \n",
       "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.067900   \n",
       "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  0.920863   \n",
       "22  NeuralNetworkClassifier         LassoRegression         False  1.067900   \n",
       "23  NeuralNetworkClassifier         LassoRegression          True  0.920863   \n",
       "\n",
       "      Tau_DR  \n",
       "0  -1.995883  \n",
       "1  -2.004188  \n",
       "2  -1.951904  \n",
       "3  -1.958615  \n",
       "4  -2.220714  \n",
       "5  -2.090103  \n",
       "6  -2.036195  \n",
       "7  -2.042983  \n",
       "8  -1.843984  \n",
       "9        NaN  \n",
       "10 -1.800005  \n",
       "11       NaN  \n",
       "12 -2.068815  \n",
       "13       NaN  \n",
       "14 -1.884296  \n",
       "15       NaN  \n",
       "16 -1.644606  \n",
       "17 -1.913626  \n",
       "18 -1.600627  \n",
       "19 -1.868053  \n",
       "20 -1.869437  \n",
       "21 -1.999540  \n",
       "22 -1.684918  \n",
       "23 -1.952421  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression         False  0.896997   \n",
      "1        LogisticRegression        LinearRegression          True  0.895288   \n",
      "2        LogisticRegression   RandomForestRegressor         False  0.896997   \n",
      "3        LogisticRegression   RandomForestRegressor          True  0.895288   \n",
      "4        LogisticRegression  NeuralNetworkRegressor         False  0.896997   \n",
      "5        LogisticRegression  NeuralNetworkRegressor          True  0.895288   \n",
      "6        LogisticRegression         LassoRegression         False  0.896997   \n",
      "7        LogisticRegression         LassoRegression          True  0.895288   \n",
      "8    RandomForestClassifier        LinearRegression         False  0.912058   \n",
      "9    RandomForestClassifier        LinearRegression          True -3.515407   \n",
      "10   RandomForestClassifier   RandomForestRegressor         False  0.912058   \n",
      "11   RandomForestClassifier   RandomForestRegressor          True -3.515407   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.912058   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor          True -3.515407   \n",
      "14   RandomForestClassifier         LassoRegression         False  0.912058   \n",
      "15   RandomForestClassifier         LassoRegression          True -3.515407   \n",
      "16  NeuralNetworkClassifier        LinearRegression         False  1.273571   \n",
      "17  NeuralNetworkClassifier        LinearRegression          True  1.103739   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.273571   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor          True  1.103739   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.273571   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  1.103739   \n",
      "22  NeuralNetworkClassifier         LassoRegression         False  1.273571   \n",
      "23  NeuralNetworkClassifier         LassoRegression          True  1.103739   \n",
      "\n",
      "      Tau_DR  \n",
      "0  -1.103917  \n",
      "1  -1.106173  \n",
      "2  -1.058235  \n",
      "3  -1.059637  \n",
      "4  -1.326090  \n",
      "5  -1.180861  \n",
      "6  -1.125998  \n",
      "7  -1.128644  \n",
      "8  -1.016369  \n",
      "9  -9.518771  \n",
      "10 -0.987425  \n",
      "11 -9.780664  \n",
      "12 -1.229146  \n",
      "13 -9.763415  \n",
      "14 -1.037920  \n",
      "15 -9.635466  \n",
      "16 -0.917413  \n",
      "17 -1.002065  \n",
      "18 -0.869977  \n",
      "19 -0.957053  \n",
      "20 -1.164582  \n",
      "21 -1.081366  \n",
      "22 -0.940831  \n",
      "23 -1.024665  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and preprocess data\n",
    "#-------------------------------------------\n",
    "census = pd.read_csv(\"census2000.csv\")\n",
    "census.rename(columns=lambda c: c[1:] if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "# Treatment indicator: T = 1 if male, 0 if female\n",
    "census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "# Outcome: log wage = log(income/hours)\n",
    "census['wage'] = census['income'] / census['hours']\n",
    "census['Y'] = np.log(census['wage'])\n",
    "\n",
    "# Define features (excluding income, hours, sex, wage, Y)\n",
    "# Here we use age, marital, race, education as features\n",
    "# Convert categorical variables to dummies\n",
    "X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "Y = census['Y'].values\n",
    "T = census['T'].values\n",
    "\n",
    "# Proportion treated\n",
    "p_hat_overall = T.mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "# Functions for first-step estimation\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit propensity score model.\n",
    "    model: classifier with predict_proba\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.fit(X, T)\n",
    "    p_hat_x = model.predict_proba(X)[:,1]\n",
    "    # Truncate probability estimates\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    return model, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit outcome regression model for the control group only.\n",
    "    model: regressor with predict\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    # Check if there are control units\n",
    "    if np.sum(T == 0) == 0:\n",
    "        raise ValueError(\"No control units in the training fold.\")\n",
    "    model.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = model.predict(X)\n",
    "    \n",
    "    # Check for NaNs or infinite values\n",
    "    if np.any(np.isnan(mu0_hat_x)) or np.any(np.isinf(mu0_hat_x)):\n",
    "        raise ValueError(\"mu0_hat_x contains NaNs or infinite values.\")\n",
    "    \n",
    "    return model, mu0_hat_x\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimators\n",
    "#-------------------------------------------\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with mu1_hat = (1/n)*sum(T_i * Y_i / p_hat_x_i)\n",
    "    and mu0_hat = (1/n)*sum{ (1-T_i)*p_hat_x_i*Y_i / [ (1 - p_hat_x_i) ] }\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat_x) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Doubly robust estimator of ATT:\n",
    "    tau_DR = (1/n)*sum( T_i * Y_i / p_hat_x_i ) \n",
    "             - (1/n)*sum( T_i * mu0_hat_x_i / p_hat_x_i + (1 - T_i) * p_hat_x_i * Y_i / (1 - p_hat_x_i) )\n",
    "    \"\"\"\n",
    "    term1 = np.mean(T * Y / p_hat_x)\n",
    "    term2 = np.mean(T * mu0_hat_x / p_hat_x + (1 - T) * p_hat_x * Y / (1 - p_hat_x))\n",
    "    tau_DR = term1 - term2\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting option\n",
    "#-------------------------------------------\n",
    "\n",
    "def cross_fit_estimates(X, Y, T, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Perform cross-fitting: split data into stratified folds, \n",
    "    estimate nuisance functions on one fold and evaluate on the other.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, T):\n",
    "        # Fit models on training fold\n",
    "        prop_model_fold, p_hat_x_fold = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, mu0_hat_x_fold = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    # Ensure p_hat_x is clipped properly\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    \n",
    "    # Verify no propensity scores are at the bounds\n",
    "    assert np.all(p_hat_x >= 1e-3) and np.all(p_hat_x <= 1-1e-3), \"Propensity scores out of bounds after clipping.\"\n",
    "\n",
    "    # Compute estimators using unit-specific propensity scores\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "def single_fit_estimates(X, Y, T, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    No cross-fitting: fit on entire sample.\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    # Ensure p_hat_x is clipped properly\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    assert np.all(p_hat_x >= 1e-3) and np.all(p_hat_x <= 1-1e-3), \"Propensity scores out of bounds after clipping.\"\n",
    "    assert not np.any(np.isnan(mu0_hat_x)), \"mu0_hat_x contains NaNs.\"\n",
    "    assert not np.any(np.isinf(mu0_hat_x)), \"mu0_hat_x contains infinite values.\"\n",
    "\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Model configurations\n",
    "#-------------------------------------------\n",
    "# We consider various methods for p_hat(x) and mu0_hat(x):\n",
    "# For demonstration: logistic regression, random forest, neural net, lasso, parametric linear.\n",
    "\n",
    "propensity_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "outcome_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=1000, random_state=42),\n",
    "    \"LassoRegression\": Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Convert X to numpy for convenience\n",
    "X_np = X.values\n",
    "\n",
    "#-------------------------------------------\n",
    "# Run the analyses and show results\n",
    "#-------------------------------------------\n",
    "results = []\n",
    "\n",
    "for p_name, p_model in propensity_models.items():\n",
    "    for m_name, m_model in outcome_models.items():\n",
    "        # Without cross-fitting\n",
    "        try:\n",
    "            tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_model, m_model)\n",
    "        except ValueError as e:\n",
    "            tau_PI_no_cf, tau_DR_no_cf = np.nan, np.nan\n",
    "            print(f\"Error in single_fit_estimates with Propensity Model: {p_name}, Outcome Model: {m_name} - {e}\")\n",
    "\n",
    "        # With cross-fitting\n",
    "        try:\n",
    "            tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_model, m_model, n_splits=2)\n",
    "        except ValueError as e:\n",
    "            tau_PI_cf, tau_DR_cf = np.nan, np.nan\n",
    "            print(f\"Error in cross_fit_estimates with Propensity Model: {p_name}, Outcome Model: {m_name} - {e}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": False,\n",
    "            \"Tau_PI\": tau_PI_no_cf,\n",
    "            \"Tau_DR\": tau_DR_no_cf\n",
    "        })\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": True,\n",
    "            \"Tau_PI\": tau_PI_cf,\n",
    "            \"Tau_DR\": tau_DR_cf\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>marital</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>hours</th>\n",
       "      <th>T</th>\n",
       "      <th>wage</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>52000</td>\n",
       "      <td>2600</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>White</td>\n",
       "      <td>2.high</td>\n",
       "      <td>35000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>16.826923</td>\n",
       "      <td>2.822980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>2400</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>2.high</td>\n",
       "      <td>6100</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>1.402824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>22000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>10.576923</td>\n",
       "      <td>2.358675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>2400</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>1.232144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>5.bachs</td>\n",
       "      <td>23100</td>\n",
       "      <td>2080</td>\n",
       "      <td>0</td>\n",
       "      <td>11.105769</td>\n",
       "      <td>2.407465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>23000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>11.057692</td>\n",
       "      <td>2.403126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>32000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>2.733368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43</td>\n",
       "      <td>F</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>20000</td>\n",
       "      <td>2080</td>\n",
       "      <td>0</td>\n",
       "      <td>9.615385</td>\n",
       "      <td>2.263364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age sex   marital   race education  income  hours  T       wage         Y\n",
       "0   48   M   Married  White  3.hsgrad   52000   2600  1  20.000000  2.995732\n",
       "1   24   M  Divorced  White    2.high   35000   2080  1  16.826923  2.822980\n",
       "2   19   F    Single  Black  3.hsgrad    2400    240  0  10.000000  2.302585\n",
       "3   18   M    Single  Black    2.high    6100   1500  1   4.066667  1.402824\n",
       "4   28   M   Married  Other   4.assoc   22000   2080  1  10.576923  2.358675\n",
       "5   18   F    Single  Black  3.hsgrad    2400    700  0   3.428571  1.232144\n",
       "6   40   F    Single  Black   5.bachs   23100   2080  0  11.105769  2.407465\n",
       "7   47   M  Divorced  White   4.assoc   23000   2080  1  11.057692  2.403126\n",
       "8   37   M   Married  White   4.assoc   32000   2080  1  15.384615  2.733368\n",
       "9   43   F   Married  White   4.assoc   20000   2080  0   9.615385  2.263364"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
