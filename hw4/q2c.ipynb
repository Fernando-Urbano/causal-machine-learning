{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: divide by zero encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:77: RuntimeWarning: invalid value encountered in divide\n",
      "  mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: divide by zero encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/var/folders/03/lg9h_1xd7nd8d90d67rgwtf40000gn/T/ipykernel_9745/1006876555.py:89: RuntimeWarning: invalid value encountered in divide\n",
      "  term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
      "/opt/homebrew/anaconda3/envs/finm/lib/python3.12/site-packages/numpy/_core/_methods.py:136: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression         False  0.874867   \n",
      "1        LogisticRegression        LinearRegression          True  0.871097   \n",
      "2        LogisticRegression   RandomForestRegressor         False  0.874867   \n",
      "3        LogisticRegression   RandomForestRegressor          True  0.871097   \n",
      "4        LogisticRegression  NeuralNetworkRegressor         False  0.874867   \n",
      "5        LogisticRegression  NeuralNetworkRegressor          True  0.871097   \n",
      "6        LogisticRegression         LassoRegression         False  0.874867   \n",
      "7        LogisticRegression         LassoRegression          True  0.871097   \n",
      "8    RandomForestClassifier        LinearRegression         False  0.958338   \n",
      "9    RandomForestClassifier        LinearRegression          True       NaN   \n",
      "10   RandomForestClassifier   RandomForestRegressor         False  0.958338   \n",
      "11   RandomForestClassifier   RandomForestRegressor          True       NaN   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.958338   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor          True       NaN   \n",
      "14   RandomForestClassifier         LassoRegression         False  0.958338   \n",
      "15   RandomForestClassifier         LassoRegression          True       NaN   \n",
      "16  NeuralNetworkClassifier        LinearRegression         False  1.067900   \n",
      "17  NeuralNetworkClassifier        LinearRegression          True  0.920863   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.067900   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor          True  0.920863   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.067900   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  0.920863   \n",
      "22  NeuralNetworkClassifier         LassoRegression         False  1.067900   \n",
      "23  NeuralNetworkClassifier         LassoRegression          True  0.920863   \n",
      "\n",
      "      Tau_DR  \n",
      "0  -1.995883  \n",
      "1  -2.004188  \n",
      "2  -1.951904  \n",
      "3  -1.958615  \n",
      "4  -2.220714  \n",
      "5  -2.090103  \n",
      "6  -2.036195  \n",
      "7  -2.042983  \n",
      "8  -1.843984  \n",
      "9        NaN  \n",
      "10 -1.800005  \n",
      "11       NaN  \n",
      "12 -2.068815  \n",
      "13       NaN  \n",
      "14 -1.884296  \n",
      "15       NaN  \n",
      "16 -1.644606  \n",
      "17 -1.913626  \n",
      "18 -1.600627  \n",
      "19 -1.868053  \n",
      "20 -1.869437  \n",
      "21 -1.999540  \n",
      "22 -1.684918  \n",
      "23 -1.952421  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and preprocess data\n",
    "#-------------------------------------------\n",
    "census = pd.read_csv(\"census2000.csv\")\n",
    "census.rename(columns=lambda c: c[1:] if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "# Treatment indicator: T = 1 if male, 0 if female\n",
    "census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "# Outcome: log wage = log(income/hours)\n",
    "census['wage'] = census['income'] / census['hours']\n",
    "census['Y'] = np.log(census['wage'])\n",
    "\n",
    "# Define features (excluding income, hours, sex, wage, Y)\n",
    "# Here we use age, marital, race, education as features\n",
    "# Convert categorical variables to dummies\n",
    "X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "Y = census['Y'].values\n",
    "T = census['T'].values\n",
    "\n",
    "# Proportion treated\n",
    "p_hat = T.mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "# Functions for first-step estimation\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit propensity score model.\n",
    "    model: classifier with predict_proba\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.fit(X, T)\n",
    "    p_hat_x = model.predict_proba(X)[:,1]\n",
    "    # truncate probability estimates\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    return model, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit outcome regression model for the control group only.\n",
    "    model: regressor with predict\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    # Fit only on controls\n",
    "    model.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = model.predict(X)\n",
    "    return model, mu0_hat_x\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimators\n",
    "#-------------------------------------------\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with mu1_hat = (1/n)*sum(T_i * Y_i / p_hat)\n",
    "    and mu0_hat = (1/n)*sum{ (1-T_i)*p_hat(X_i)*Y_i / [ (1 - p_hat(X_i)) ] }\n",
    "\n",
    "    p_hat is the average of T (overall proportion treated).\n",
    "    p_hat_x is the estimated propensity for each unit.\n",
    "    mu0_hat_x is the estimated outcome regression for the control condition.\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Doubly robust estimator of ATT:\n",
    "    tau_DR = (1/n)*sum( t_i * y_i / p_hat ) \n",
    "             - (1/p_hat)* (1/n)* sum( t_i * mu0_hat(x_i) + (1-t_i)*p_hat(x_i)*Y_i/(1 - p_hat(x_i)) )\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    term1 = np.mean(T * Y / p_hat)\n",
    "    term2 = (1/(p_hat)) * np.mean( T*mu0_hat_x + (1 - T)*p_hat_x*Y/(1 - p_hat_x) )\n",
    "    tau_DR = term1 - term2\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting option\n",
    "#-------------------------------------------\n",
    "\n",
    "def cross_fit_estimates(X, Y, T, p_hat, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Perform cross-fitting: split data into folds, \n",
    "    estimate nuisance functions on one fold and evaluate on the other.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        # Fit models on training fold\n",
    "        prop_model_fold, _ = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, _ = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    # Compute estimators\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "def single_fit_estimates(X, Y, T, p_hat, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    No cross-fitting: fit on entire sample.\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Model configurations\n",
    "#-------------------------------------------\n",
    "# We consider various methods for p_hat(x) and mu0_hat(x):\n",
    "# For demonstration: logistic regression, random forest, neural net, lasso, parametric linear.\n",
    "\n",
    "propensity_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "outcome_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=500, random_state=42),\n",
    "    \"LassoRegression\": Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Convert X to numpy for convenience\n",
    "X_np = X.values\n",
    "\n",
    "#-------------------------------------------\n",
    "# Run the analyses and show results\n",
    "#-------------------------------------------\n",
    "results = []\n",
    "\n",
    "for p_name, p_model in propensity_models.items():\n",
    "    for m_name, m_model in outcome_models.items():\n",
    "        # Without cross-fitting\n",
    "        tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_hat, p_model, m_model)\n",
    "\n",
    "        # With cross-fitting\n",
    "        tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_hat, p_model, m_model, n_splits=2)\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": False,\n",
    "            \"Tau_PI\": tau_PI_no_cf,\n",
    "            \"Tau_DR\": tau_DR_no_cf\n",
    "        })\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": True,\n",
    "            \"Tau_PI\": tau_PI_cf,\n",
    "            \"Tau_DR\": tau_DR_cf\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and print\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# This results_df now contains the ATT estimates for both the PI and DR estimators,\n",
    "# with and without cross-fitting, and for different first-step models.\n",
    "# You can further analyze, plot, or discuss these results as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age sex   marital   race education  income  hours\n",
      "0       48   M   Married  White  3.hsgrad   52000   2600\n",
      "1       24   M  Divorced  White    2.high   35000   2080\n",
      "2       19   F    Single  Black  3.hsgrad    2400    240\n",
      "3       18   M    Single  Black    2.high    6100   1500\n",
      "4       28   M   Married  Other   4.assoc   22000   2080\n",
      "...    ...  ..       ...    ...       ...     ...    ...\n",
      "31397   37   F  Divorced  White   4.assoc   18700   2080\n",
      "31398   26   F   Married  White   5.bachs   25000   1600\n",
      "31399   28   M   Married  White   5.bachs   34000   2550\n",
      "31400   44   M   Married  White  3.hsgrad   32900   2484\n",
      "31401   44   F   Married  White   4.assoc    8000   1248\n",
      "\n",
      "[31402 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "census = pd.read_csv(\"census2000.csv\")\n",
    "print(census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"q2c-data/preliminary_2_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Propensity_Model</th>\n",
       "      <th>Outcome_Model</th>\n",
       "      <th>CrossFitting</th>\n",
       "      <th>Tau_PI</th>\n",
       "      <th>Tau_DR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-1.995883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.004188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-1.951904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-1.958615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-2.220714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.090103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.874867</td>\n",
       "      <td>-2.036195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.871097</td>\n",
       "      <td>-2.042983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.843984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.800005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-2.068815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>0.958338</td>\n",
       "      <td>-1.884296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.644606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.913626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.600627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.868053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.869437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>NeuralNetworkRegressor</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.999540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>False</td>\n",
       "      <td>1.067900</td>\n",
       "      <td>-1.684918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeuralNetworkClassifier</td>\n",
       "      <td>LassoRegression</td>\n",
       "      <td>True</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>-1.952421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
       "0        LogisticRegression        LinearRegression         False  0.874867   \n",
       "1        LogisticRegression        LinearRegression          True  0.871097   \n",
       "2        LogisticRegression   RandomForestRegressor         False  0.874867   \n",
       "3        LogisticRegression   RandomForestRegressor          True  0.871097   \n",
       "4        LogisticRegression  NeuralNetworkRegressor         False  0.874867   \n",
       "5        LogisticRegression  NeuralNetworkRegressor          True  0.871097   \n",
       "6        LogisticRegression         LassoRegression         False  0.874867   \n",
       "7        LogisticRegression         LassoRegression          True  0.871097   \n",
       "8    RandomForestClassifier        LinearRegression         False  0.958338   \n",
       "9    RandomForestClassifier        LinearRegression          True       NaN   \n",
       "10   RandomForestClassifier   RandomForestRegressor         False  0.958338   \n",
       "11   RandomForestClassifier   RandomForestRegressor          True       NaN   \n",
       "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.958338   \n",
       "13   RandomForestClassifier  NeuralNetworkRegressor          True       NaN   \n",
       "14   RandomForestClassifier         LassoRegression         False  0.958338   \n",
       "15   RandomForestClassifier         LassoRegression          True       NaN   \n",
       "16  NeuralNetworkClassifier        LinearRegression         False  1.067900   \n",
       "17  NeuralNetworkClassifier        LinearRegression          True  0.920863   \n",
       "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.067900   \n",
       "19  NeuralNetworkClassifier   RandomForestRegressor          True  0.920863   \n",
       "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.067900   \n",
       "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  0.920863   \n",
       "22  NeuralNetworkClassifier         LassoRegression         False  1.067900   \n",
       "23  NeuralNetworkClassifier         LassoRegression          True  0.920863   \n",
       "\n",
       "      Tau_DR  \n",
       "0  -1.995883  \n",
       "1  -2.004188  \n",
       "2  -1.951904  \n",
       "3  -1.958615  \n",
       "4  -2.220714  \n",
       "5  -2.090103  \n",
       "6  -2.036195  \n",
       "7  -2.042983  \n",
       "8  -1.843984  \n",
       "9        NaN  \n",
       "10 -1.800005  \n",
       "11       NaN  \n",
       "12 -2.068815  \n",
       "13       NaN  \n",
       "14 -1.884296  \n",
       "15       NaN  \n",
       "16 -1.644606  \n",
       "17 -1.913626  \n",
       "18 -1.600627  \n",
       "19 -1.868053  \n",
       "20 -1.869437  \n",
       "21 -1.999540  \n",
       "22 -1.684918  \n",
       "23 -1.952421  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  CrossFitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression         False  0.896997   \n",
      "1        LogisticRegression        LinearRegression          True  0.895288   \n",
      "2        LogisticRegression   RandomForestRegressor         False  0.896997   \n",
      "3        LogisticRegression   RandomForestRegressor          True  0.895288   \n",
      "4        LogisticRegression  NeuralNetworkRegressor         False  0.896997   \n",
      "5        LogisticRegression  NeuralNetworkRegressor          True  0.895288   \n",
      "6        LogisticRegression         LassoRegression         False  0.896997   \n",
      "7        LogisticRegression         LassoRegression          True  0.895288   \n",
      "8    RandomForestClassifier        LinearRegression         False  0.912058   \n",
      "9    RandomForestClassifier        LinearRegression          True -3.515407   \n",
      "10   RandomForestClassifier   RandomForestRegressor         False  0.912058   \n",
      "11   RandomForestClassifier   RandomForestRegressor          True -3.515407   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor         False  0.912058   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor          True -3.515407   \n",
      "14   RandomForestClassifier         LassoRegression         False  0.912058   \n",
      "15   RandomForestClassifier         LassoRegression          True -3.515407   \n",
      "16  NeuralNetworkClassifier        LinearRegression         False  1.273571   \n",
      "17  NeuralNetworkClassifier        LinearRegression          True  1.103739   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor         False  1.273571   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor          True  1.103739   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor         False  1.273571   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor          True  1.103739   \n",
      "22  NeuralNetworkClassifier         LassoRegression         False  1.273571   \n",
      "23  NeuralNetworkClassifier         LassoRegression          True  1.103739   \n",
      "\n",
      "      Tau_DR  \n",
      "0  -1.103917  \n",
      "1  -1.106173  \n",
      "2  -1.058235  \n",
      "3  -1.059637  \n",
      "4  -1.326090  \n",
      "5  -1.180861  \n",
      "6  -1.125998  \n",
      "7  -1.128644  \n",
      "8  -1.016369  \n",
      "9  -9.518771  \n",
      "10 -0.987425  \n",
      "11 -9.780664  \n",
      "12 -1.229146  \n",
      "13 -9.763415  \n",
      "14 -1.037920  \n",
      "15 -9.635466  \n",
      "16 -0.917413  \n",
      "17 -1.002065  \n",
      "18 -0.869977  \n",
      "19 -0.957053  \n",
      "20 -1.164582  \n",
      "21 -1.081366  \n",
      "22 -0.940831  \n",
      "23 -1.024665  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import clone\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and preprocess data\n",
    "#-------------------------------------------\n",
    "census = pd.read_csv(\"census2000.csv\")\n",
    "census.rename(columns=lambda c: c[1:] if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "# Treatment indicator: T = 1 if male, 0 if female\n",
    "census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "# Outcome: log wage = log(income/hours)\n",
    "census['wage'] = census['income'] / census['hours']\n",
    "census['Y'] = np.log(census['wage'])\n",
    "\n",
    "# Define features (excluding income, hours, sex, wage, Y)\n",
    "# Here we use age, marital, race, education as features\n",
    "# Convert categorical variables to dummies\n",
    "X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "Y = census['Y'].values\n",
    "T = census['T'].values\n",
    "\n",
    "# Proportion treated\n",
    "p_hat_overall = T.mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "# Functions for first-step estimation\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit propensity score model.\n",
    "    model: classifier with predict_proba\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    model.fit(X, T)\n",
    "    p_hat_x = model.predict_proba(X)[:,1]\n",
    "    # Truncate probability estimates\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    return model, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit outcome regression model for the control group only.\n",
    "    model: regressor with predict\n",
    "    \"\"\"\n",
    "    model = clone(model)\n",
    "    # Check if there are control units\n",
    "    if np.sum(T == 0) == 0:\n",
    "        raise ValueError(\"No control units in the training fold.\")\n",
    "    model.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = model.predict(X)\n",
    "    \n",
    "    # Check for NaNs or infinite values\n",
    "    if np.any(np.isnan(mu0_hat_x)) or np.any(np.isinf(mu0_hat_x)):\n",
    "        raise ValueError(\"mu0_hat_x contains NaNs or infinite values.\")\n",
    "    \n",
    "    return model, mu0_hat_x\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimators\n",
    "#-------------------------------------------\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with mu1_hat = (1/n)*sum(T_i * Y_i / p_hat_x_i)\n",
    "    and mu0_hat = (1/n)*sum{ (1-T_i)*p_hat_x_i*Y_i / [ (1 - p_hat_x_i) ] }\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat_x) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Doubly robust estimator of ATT:\n",
    "    tau_DR = (1/n)*sum( T_i * Y_i / p_hat_x_i ) \n",
    "             - (1/n)*sum( T_i * mu0_hat_x_i / p_hat_x_i + (1 - T_i) * p_hat_x_i * Y_i / (1 - p_hat_x_i) )\n",
    "    \"\"\"\n",
    "    term1 = np.mean(T * Y / p_hat_x)\n",
    "    term2 = np.mean(T * mu0_hat_x / p_hat_x + (1 - T) * p_hat_x * Y / (1 - p_hat_x))\n",
    "    tau_DR = term1 - term2\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting option\n",
    "#-------------------------------------------\n",
    "\n",
    "def cross_fit_estimates(X, Y, T, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Perform cross-fitting: split data into stratified folds, \n",
    "    estimate nuisance functions on one fold and evaluate on the other.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, T):\n",
    "        # Fit models on training fold\n",
    "        prop_model_fold, p_hat_x_fold = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, mu0_hat_x_fold = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    # Ensure p_hat_x is clipped properly\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    \n",
    "    # Verify no propensity scores are at the bounds\n",
    "    assert np.all(p_hat_x >= 1e-3) and np.all(p_hat_x <= 1-1e-3), \"Propensity scores out of bounds after clipping.\"\n",
    "\n",
    "    # Compute estimators using unit-specific propensity scores\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "def single_fit_estimates(X, Y, T, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    No cross-fitting: fit on entire sample.\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    # Ensure p_hat_x is clipped properly\n",
    "    p_hat_x = np.clip(p_hat_x, 1e-3, 1-1e-3)\n",
    "    assert np.all(p_hat_x >= 1e-3) and np.all(p_hat_x <= 1-1e-3), \"Propensity scores out of bounds after clipping.\"\n",
    "    assert not np.any(np.isnan(mu0_hat_x)), \"mu0_hat_x contains NaNs.\"\n",
    "    assert not np.any(np.isinf(mu0_hat_x)), \"mu0_hat_x contains infinite values.\"\n",
    "\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Model configurations\n",
    "#-------------------------------------------\n",
    "# We consider various methods for p_hat(x) and mu0_hat(x):\n",
    "# For demonstration: logistic regression, random forest, neural net, lasso, parametric linear.\n",
    "\n",
    "propensity_models = {\n",
    "    \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=1000),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "outcome_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=1000, random_state=42),\n",
    "    \"LassoRegression\": Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "# Convert X to numpy for convenience\n",
    "X_np = X.values\n",
    "\n",
    "#-------------------------------------------\n",
    "# Run the analyses and show results\n",
    "#-------------------------------------------\n",
    "results = []\n",
    "\n",
    "for p_name, p_model in propensity_models.items():\n",
    "    for m_name, m_model in outcome_models.items():\n",
    "        # Without cross-fitting\n",
    "        try:\n",
    "            tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_model, m_model)\n",
    "        except ValueError as e:\n",
    "            tau_PI_no_cf, tau_DR_no_cf = np.nan, np.nan\n",
    "            print(f\"Error in single_fit_estimates with Propensity Model: {p_name}, Outcome Model: {m_name} - {e}\")\n",
    "\n",
    "        # With cross-fitting\n",
    "        try:\n",
    "            tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_model, m_model, n_splits=2)\n",
    "        except ValueError as e:\n",
    "            tau_PI_cf, tau_DR_cf = np.nan, np.nan\n",
    "            print(f\"Error in cross_fit_estimates with Propensity Model: {p_name}, Outcome Model: {m_name} - {e}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": False,\n",
    "            \"Tau_PI\": tau_PI_no_cf,\n",
    "            \"Tau_DR\": tau_DR_no_cf\n",
    "        })\n",
    "\n",
    "        results.append({\n",
    "            \"Propensity_Model\": p_name,\n",
    "            \"Outcome_Model\": m_name,\n",
    "            \"CrossFitting\": True,\n",
    "            \"Tau_PI\": tau_PI_cf,\n",
    "            \"Tau_DR\": tau_DR_cf\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>marital</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>income</th>\n",
       "      <th>hours</th>\n",
       "      <th>T</th>\n",
       "      <th>wage</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>52000</td>\n",
       "      <td>2600</td>\n",
       "      <td>1</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.995732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>White</td>\n",
       "      <td>2.high</td>\n",
       "      <td>35000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>16.826923</td>\n",
       "      <td>2.822980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>2400</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>2.high</td>\n",
       "      <td>6100</td>\n",
       "      <td>1500</td>\n",
       "      <td>1</td>\n",
       "      <td>4.066667</td>\n",
       "      <td>1.402824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>Other</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>22000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>10.576923</td>\n",
       "      <td>2.358675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>3.hsgrad</td>\n",
       "      <td>2400</td>\n",
       "      <td>700</td>\n",
       "      <td>0</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>1.232144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Black</td>\n",
       "      <td>5.bachs</td>\n",
       "      <td>23100</td>\n",
       "      <td>2080</td>\n",
       "      <td>0</td>\n",
       "      <td>11.105769</td>\n",
       "      <td>2.407465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>23000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>11.057692</td>\n",
       "      <td>2.403126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>32000</td>\n",
       "      <td>2080</td>\n",
       "      <td>1</td>\n",
       "      <td>15.384615</td>\n",
       "      <td>2.733368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43</td>\n",
       "      <td>F</td>\n",
       "      <td>Married</td>\n",
       "      <td>White</td>\n",
       "      <td>4.assoc</td>\n",
       "      <td>20000</td>\n",
       "      <td>2080</td>\n",
       "      <td>0</td>\n",
       "      <td>9.615385</td>\n",
       "      <td>2.263364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age sex   marital   race education  income  hours  T       wage         Y\n",
       "0   48   M   Married  White  3.hsgrad   52000   2600  1  20.000000  2.995732\n",
       "1   24   M  Divorced  White    2.high   35000   2080  1  16.826923  2.822980\n",
       "2   19   F    Single  Black  3.hsgrad    2400    240  0  10.000000  2.302585\n",
       "3   18   M    Single  Black    2.high    6100   1500  1   4.066667  1.402824\n",
       "4   28   M   Married  Other   4.assoc   22000   2080  1  10.576923  2.358675\n",
       "5   18   F    Single  Black  3.hsgrad    2400    700  0   3.428571  1.232144\n",
       "6   40   F    Single  Black   5.bachs   23100   2080  0  11.105769  2.407465\n",
       "7   47   M  Divorced  White   4.assoc   23000   2080  1  11.057692  2.403126\n",
       "8   37   M   Married  White   4.assoc   32000   2080  1  15.384615  2.733368\n",
       "9   43   F   Married  White   4.assoc   20000   2080  0   9.615385  2.263364"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  Cross_Fitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression          False  0.896997   \n",
      "1        LogisticRegression        LinearRegression           True  0.895288   \n",
      "2        LogisticRegression   RandomForestRegressor          False  0.896997   \n",
      "3        LogisticRegression   RandomForestRegressor           True  0.895288   \n",
      "4        LogisticRegression  NeuralNetworkRegressor          False  0.896997   \n",
      "5        LogisticRegression  NeuralNetworkRegressor           True  0.895288   \n",
      "6        LogisticRegression         LassoRegression          False  0.896997   \n",
      "7        LogisticRegression         LassoRegression           True  0.895288   \n",
      "8    RandomForestClassifier        LinearRegression          False  0.912058   \n",
      "9    RandomForestClassifier        LinearRegression           True -3.515407   \n",
      "10   RandomForestClassifier   RandomForestRegressor          False  0.912058   \n",
      "11   RandomForestClassifier   RandomForestRegressor           True -3.515407   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor          False  0.912058   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor           True -3.515407   \n",
      "14   RandomForestClassifier         LassoRegression          False  0.912058   \n",
      "15   RandomForestClassifier         LassoRegression           True -3.515407   \n",
      "16  NeuralNetworkClassifier        LinearRegression          False  1.273571   \n",
      "17  NeuralNetworkClassifier        LinearRegression           True  1.103739   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor          False  1.273571   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor           True  1.103739   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor          False  1.273571   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor           True  1.103739   \n",
      "22  NeuralNetworkClassifier         LassoRegression          False  1.273571   \n",
      "23  NeuralNetworkClassifier         LassoRegression           True  1.103739   \n",
      "\n",
      "       Tau_DR  \n",
      "0   -1.973753  \n",
      "1   -1.977135  \n",
      "2   -1.929774  \n",
      "3   -1.931290  \n",
      "4   -2.198584  \n",
      "5   -2.055299  \n",
      "6   -2.014065  \n",
      "7   -2.018228  \n",
      "8   -1.890263  \n",
      "9  -12.688544  \n",
      "10  -1.846285  \n",
      "11 -12.642699  \n",
      "12  -2.115095  \n",
      "13 -12.766709  \n",
      "14  -1.930576  \n",
      "15 -12.729637  \n",
      "16  -1.438935  \n",
      "17  -1.679327  \n",
      "18  -1.394956  \n",
      "19  -1.633482  \n",
      "20  -1.663766  \n",
      "21  -1.757492  \n",
      "22  -1.479247  \n",
      "23  -1.720420  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and Preprocess Data\n",
    "#-------------------------------------------\n",
    "def load_and_preprocess_census(file_path):\n",
    "    census = pd.read_csv(file_path)\n",
    "    # Rename columns with leading spaces\n",
    "    census.rename(columns=lambda c: c.strip() if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "    # Treatment indicator: T = 1 if male, 0 if female\n",
    "    census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "    # Outcome: log wage = log(income/hours)\n",
    "    census['wage'] = census['income'] / census['hours']\n",
    "    census['Y'] = np.log(census['wage'])\n",
    "\n",
    "    # Features: exclude sex, wage, Y, income, hours\n",
    "    # Here we use age, marital, race, education\n",
    "    X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "    Y = census['Y'].values\n",
    "    T = census['T'].values\n",
    "\n",
    "    return X, Y, T\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimation Functions\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit the propensity score model P(T=1|X) and return fitted model & predictions.\n",
    "    \"\"\"\n",
    "    # Clone to avoid modifying the original model\n",
    "    m = clone(model)\n",
    "    m.fit(X, T)\n",
    "    p_hat_x = m.predict_proba(X)[:,1]\n",
    "    return m, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit the outcome model for the control group only: E[Y|X,T=0].\n",
    "    \"\"\"\n",
    "    m = clone(model)\n",
    "    # Fit only on controls\n",
    "    m.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = m.predict(X)\n",
    "    return m, mu0_hat_x\n",
    "\n",
    "def truncate_scores(scores, lower=1e-3, upper=1 - 1e-3):\n",
    "    return np.clip(scores, lower, upper)\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with:\n",
    "    mu1_hat = (1/n)*sum(T_i * Y_i / p_hat_x_i)\n",
    "    mu0_hat = (1/n)*sum((1 - T_i)*p_hat_x_i*Y_i / (1 - p_hat_x_i))\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat_x) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    p_hat = T.mean()\n",
    "    term1 = np.mean(T * Y / p_hat_x)\n",
    "    # Correct: no division by p_hat_x for the T * mu0_hat_x term\n",
    "    term2 = np.mean(T * mu0_hat_x + ((1 - T) * p_hat_x * Y / (1 - p_hat_x)))\n",
    "    tau_DR = term1 - (term2 / p_hat)\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting (optional)\n",
    "#-------------------------------------------\n",
    "def cross_fit_estimates(X, Y, T, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Optional cross-fitting: split data and estimate nuisance functions in a fold-specific manner.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, T):\n",
    "        # Fit on training fold\n",
    "        prop_model_fold, _ = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, _ = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    p_hat_x = truncate_scores(p_hat_x)\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "\n",
    "def single_fit_estimates(X, Y, T, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    Fit on entire data (no cross-fitting).\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    p_hat_x = truncate_scores(p_hat_x)\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Example Usage\n",
    "#-------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X, Y, T = load_and_preprocess_census(\"census2000.csv\")\n",
    "    X_np = X.values\n",
    "\n",
    "    # Define various models for propensity and outcome\n",
    "    propensity_models = {\n",
    "        \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=2000),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=2000, random_state=42)\n",
    "    }\n",
    "\n",
    "    outcome_models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=2000, random_state=42),\n",
    "        \"LassoRegression\": Lasso(alpha=0.1, max_iter=2000)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for p_name, p_model in propensity_models.items():\n",
    "        for m_name, m_model in outcome_models.items():\n",
    "            # Without cross-fitting\n",
    "            tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_model, m_model)\n",
    "\n",
    "            # With cross-fitting\n",
    "            tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_model, m_model, n_splits=2)\n",
    "\n",
    "            results.append({\n",
    "                \"Propensity_Model\": p_name,\n",
    "                \"Outcome_Model\": m_name,\n",
    "                \"Cross_Fitting\": False,\n",
    "                \"Tau_PI\": tau_PI_no_cf,\n",
    "                \"Tau_DR\": tau_DR_no_cf\n",
    "            })\n",
    "            results.append({\n",
    "                \"Propensity_Model\": p_name,\n",
    "                \"Outcome_Model\": m_name,\n",
    "                \"Cross_Fitting\": True,\n",
    "                \"Tau_PI\": tau_PI_cf,\n",
    "                \"Tau_DR\": tau_DR_cf\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Propensity_Model           Outcome_Model  Cross_Fitting    Tau_PI  \\\n",
      "0        LogisticRegression        LinearRegression          False  1.366433   \n",
      "1        LogisticRegression        LinearRegression           True  1.363280   \n",
      "2        LogisticRegression   RandomForestRegressor          False  1.366433   \n",
      "3        LogisticRegression   RandomForestRegressor           True  1.363280   \n",
      "4        LogisticRegression  NeuralNetworkRegressor          False  1.366433   \n",
      "5        LogisticRegression  NeuralNetworkRegressor           True  1.363280   \n",
      "6        LogisticRegression         LassoRegression          False  1.366433   \n",
      "7        LogisticRegression         LassoRegression           True  1.363280   \n",
      "8    RandomForestClassifier        LinearRegression          False  1.371529   \n",
      "9    RandomForestClassifier        LinearRegression           True -0.650575   \n",
      "10   RandomForestClassifier   RandomForestRegressor          False  1.371529   \n",
      "11   RandomForestClassifier   RandomForestRegressor           True -0.650575   \n",
      "12   RandomForestClassifier  NeuralNetworkRegressor          False  1.371529   \n",
      "13   RandomForestClassifier  NeuralNetworkRegressor           True -0.650575   \n",
      "14   RandomForestClassifier         LassoRegression          False  1.371529   \n",
      "15   RandomForestClassifier         LassoRegression           True -0.650575   \n",
      "16  NeuralNetworkClassifier        LinearRegression          False  1.509397   \n",
      "17  NeuralNetworkClassifier        LinearRegression           True  1.424044   \n",
      "18  NeuralNetworkClassifier   RandomForestRegressor          False  1.509397   \n",
      "19  NeuralNetworkClassifier   RandomForestRegressor           True  1.424044   \n",
      "20  NeuralNetworkClassifier  NeuralNetworkRegressor          False  1.509397   \n",
      "21  NeuralNetworkClassifier  NeuralNetworkRegressor           True  1.424044   \n",
      "22  NeuralNetworkClassifier         LassoRegression          False  1.509397   \n",
      "23  NeuralNetworkClassifier         LassoRegression           True  1.424044   \n",
      "\n",
      "       Tau_DR  \n",
      "0   -2.226286  \n",
      "1   -2.234279  \n",
      "2   -2.230346  \n",
      "3   -2.238295  \n",
      "4   -2.173630  \n",
      "5   -2.255142  \n",
      "6   -2.251588  \n",
      "7   -2.259363  \n",
      "8   -2.141202  \n",
      "9  -10.374873  \n",
      "10  -2.145262  \n",
      "11 -10.378888  \n",
      "12  -2.088546  \n",
      "13 -10.395736  \n",
      "14  -2.166504  \n",
      "15 -10.399956  \n",
      "16  -2.020649  \n",
      "17  -2.146075  \n",
      "18  -2.024709  \n",
      "19  -2.150091  \n",
      "20  -1.967993  \n",
      "21  -2.166938  \n",
      "22  -2.045951  \n",
      "23  -2.171158  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#-------------------------------------------\n",
    "# Load and Preprocess Data\n",
    "#-------------------------------------------\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(31380)\n",
    "\n",
    "def load_and_preprocess_census(file_path):\n",
    "    census = pd.read_csv(file_path)\n",
    "    census.rename(columns=lambda c: c.strip() if c.startswith(' ') else c, inplace=True)\n",
    "\n",
    "    # Filter data similar to the R code:\n",
    "    # hours > 500, income > 5000, age < 60\n",
    "    census = census[(census['hours'] > 500) & (census['income'] > 5000) & (census['age'] < 60)]\n",
    "\n",
    "    # Treatment: T = 1 if male, 0 if female\n",
    "    census['T'] = (census['sex'] == 'M').astype(int)\n",
    "\n",
    "    # Outcome: log wage = log(income/hours)\n",
    "    census['wage'] = census['income'] / census['hours']\n",
    "    census['Y'] = np.log(census['wage'])\n",
    "\n",
    "    # Releveling factors: To mimic the R code, we can manually ensure \"White\" and \"Married\" are baselines.\n",
    "    # We'll make sure that 'race' and 'marital' have White and Married as the baseline by ordering categories:\n",
    "    census['race'] = pd.Categorical(census['race'], categories=[\"White\", \"Asian\", \"Black\", \"NativeAmerican\", \"Other\"], ordered=False)\n",
    "    census['marital'] = pd.Categorical(census['marital'], categories=[\"Married\", \"Divorced\", \"Separated\", \"Single\", \"Widow\"], ordered=False)\n",
    "\n",
    "    # Create dummies, ensuring the first category is the baseline\n",
    "    X = pd.get_dummies(census[['age','marital','race','education']], drop_first=True)\n",
    "    Y = census['Y'].values\n",
    "    T = census['T'].values\n",
    "\n",
    "    return X, Y, T\n",
    "\n",
    "#-------------------------------------------\n",
    "# Estimation Functions\n",
    "#-------------------------------------------\n",
    "\n",
    "def fit_propensity_model(model, X, T):\n",
    "    \"\"\"\n",
    "    Fit the propensity score model P(T=1|X) and return fitted model & predictions.\n",
    "    \"\"\"\n",
    "    # Clone to avoid modifying the original model\n",
    "    m = clone(model)\n",
    "    m.fit(X, T)\n",
    "    p_hat_x = m.predict_proba(X)[:,1]\n",
    "    return m, p_hat_x\n",
    "\n",
    "def fit_outcome_model(model, X, Y, T):\n",
    "    \"\"\"\n",
    "    Fit the outcome model for the control group only: E[Y|X,T=0].\n",
    "    \"\"\"\n",
    "    m = clone(model)\n",
    "    # Fit only on controls\n",
    "    m.fit(X[T==0], Y[T==0])\n",
    "    mu0_hat_x = m.predict(X)\n",
    "    return m, mu0_hat_x\n",
    "\n",
    "def truncate_scores(scores, lower=1e-3, upper=1 - 1e-3):\n",
    "    return np.clip(scores, lower, upper)\n",
    "\n",
    "def plugin_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    \"\"\"\n",
    "    Plug-in estimator of ATT:\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    with:\n",
    "    mu1_hat = (1/n)*sum(T_i * Y_i / p_hat_x_i)\n",
    "    mu0_hat = (1/n)*sum((1 - T_i)*p_hat_x_i*Y_i / (1 - p_hat_x_i))\n",
    "    \"\"\"\n",
    "    n = len(T)\n",
    "    mu1_hat = np.sum(T * Y / p_hat_x) / n\n",
    "    mu0_hat = np.sum((1 - T) * p_hat_x * Y / (1 - p_hat_x)) / n\n",
    "    tau_PI = mu1_hat - mu0_hat\n",
    "    return tau_PI\n",
    "\n",
    "def doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x):\n",
    "    p_hat = T.mean()\n",
    "    term1 = np.mean(T * Y / p_hat_x)\n",
    "    # Correct: no division by p_hat_x for the T * mu0_hat_x term\n",
    "    term2 = np.mean(T * mu0_hat_x + ((1 - T) * p_hat_x * Y / (1 - p_hat_x)))\n",
    "    tau_DR = term1 - (term2 / p_hat)\n",
    "    return tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Cross-fitting (optional)\n",
    "#-------------------------------------------\n",
    "def cross_fit_estimates(X, Y, T, prop_model, outcome_model, n_splits=2):\n",
    "    \"\"\"\n",
    "    Optional cross-fitting: split data and estimate nuisance functions in a fold-specific manner.\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    p_hat_x = np.zeros(len(T))\n",
    "    mu0_hat_x = np.zeros(len(T))\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X, T):\n",
    "        # Fit on training fold\n",
    "        prop_model_fold, _ = fit_propensity_model(prop_model, X[train_idx], T[train_idx])\n",
    "        outcome_model_fold, _ = fit_outcome_model(outcome_model, X[train_idx], Y[train_idx], T[train_idx])\n",
    "\n",
    "        # Predict on test fold\n",
    "        p_hat_x[test_idx] = prop_model_fold.predict_proba(X[test_idx])[:,1]\n",
    "        mu0_hat_x[test_idx] = outcome_model_fold.predict(X[test_idx])\n",
    "\n",
    "    p_hat_x = truncate_scores(p_hat_x)\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "\n",
    "def single_fit_estimates(X, Y, T, prop_model, outcome_model):\n",
    "    \"\"\"\n",
    "    Fit on entire data (no cross-fitting).\n",
    "    \"\"\"\n",
    "    _, p_hat_x = fit_propensity_model(prop_model, X, T)\n",
    "    _, mu0_hat_x = fit_outcome_model(outcome_model, X, Y, T)\n",
    "\n",
    "    p_hat_x = truncate_scores(p_hat_x)\n",
    "    tau_PI = plugin_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    tau_DR = doubly_robust_estimator(T, Y, p_hat_x, mu0_hat_x)\n",
    "    return tau_PI, tau_DR\n",
    "\n",
    "#-------------------------------------------\n",
    "# Example Usage\n",
    "#-------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    X, Y, T = load_and_preprocess_census(\"census2000.csv\")\n",
    "    X_np = X.values\n",
    "\n",
    "    # Define various models for propensity and outcome\n",
    "    propensity_models = {\n",
    "        \"LogisticRegression\": LogisticRegression(solver='lbfgs', max_iter=2000),\n",
    "        \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"NeuralNetworkClassifier\": MLPClassifier(hidden_layer_sizes=(32,16), max_iter=2000, random_state=42)\n",
    "    }\n",
    "\n",
    "    outcome_models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"NeuralNetworkRegressor\": MLPRegressor(hidden_layer_sizes=(32,16), max_iter=2000, random_state=42),\n",
    "        \"LassoRegression\": Lasso(alpha=0.1, max_iter=2000)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for p_name, p_model in propensity_models.items():\n",
    "        for m_name, m_model in outcome_models.items():\n",
    "            # Without cross-fitting\n",
    "            tau_PI_no_cf, tau_DR_no_cf = single_fit_estimates(X_np, Y, T, p_model, m_model)\n",
    "\n",
    "            # With cross-fitting\n",
    "            tau_PI_cf, tau_DR_cf = cross_fit_estimates(X_np, Y, T, p_model, m_model, n_splits=2)\n",
    "\n",
    "            results.append({\n",
    "                \"Propensity_Model\": p_name,\n",
    "                \"Outcome_Model\": m_name,\n",
    "                \"Cross_Fitting\": False,\n",
    "                \"Tau_PI\": tau_PI_no_cf,\n",
    "                \"Tau_DR\": tau_DR_no_cf\n",
    "            })\n",
    "            results.append({\n",
    "                \"Propensity_Model\": p_name,\n",
    "                \"Outcome_Model\": m_name,\n",
    "                \"Cross_Fitting\": True,\n",
    "                \"Tau_PI\": tau_PI_cf,\n",
    "                \"Tau_DR\": tau_DR_cf\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
