\documentclass{article}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{ifthen}
\usepackage{amsfonts}
\usepackage{colortbl}
\usepackage[table,xcdraw]{xcolor}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amssymb}
\usepackage{color}
\usepackage{lscape}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{fancyhdr}
\pagestyle{fancy}

% Define the header
\fancyfoot[R]{Fernando Urbano}
\renewcommand{\footrulewidth}{0.2pt}

\fancyhead[L]{ECMA 31380 - Causal Machine Learning}
\fancyhead[R]{Homework 3}

\usepackage{graphicx}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\newcommand{\divider}{\vspace{1em}\hrule\vspace{1em}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{Rstyle}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=true,                  
  tabsize=2,
  language=R
}

\newboolean{imagesbool:q2d}
\newboolean{imagesbool:q2m}
\newboolean{imagesbool:q3f}
\newboolean{imagesbool:q3g}
\newboolean{imagesbool:q3i}
\newboolean{imagesbool:q3j}
\newboolean{showcomments}

\setboolean{showcomments}{false}
\setboolean{imagesbool:q2d}{true}
\setboolean{imagesbool:q2m}{true}
\setboolean{imagesbool:q3f}{true}
\setboolean{imagesbool:q3g}{true}
\setboolean{imagesbool:q3i}{true}
\setboolean{imagesbool:q3j}{true}

\lstdefinestyle{RstyleComment}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{codegreen},
  stringstyle=\color{codegreen},
  basicstyle=\ttfamily\footnotesize\color{codegreen},
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=true,                  
  tabsize=2,
  language=R
}

\lstdefinestyle{RstyleCommentSmall}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{codegreen},
  stringstyle=\color{codegreen},
  basicstyle=\ttfamily\scriptsize\color{codegreen}, % Changed font size here
  breakatwhitespace=false,          
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=true,                  
  tabsize=2,
  language=R
}

\lstdefinestyle{RstyleCommentTiny}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegreen},
  keywordstyle=\color{codegreen},
  stringstyle=\color{codegreen},
  basicstyle=\ttfamily\tiny\color{codegreen}, % Changed to \tiny
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=true,                  
  tabsize=2,
  language=R
}

\title{ECMA 31380 - Causal Machine Learning - Homework 3}
\author{Fernando Rocha Urbano}
\date{Autumn 2024}

% Define colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Setup the listings package
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\newenvironment{colorparagraph}[1]{\par\color{#1}}{\par}
\definecolor{questioncolor}{RGB}{20, 40, 150}
\definecolor{tacolor}{RGB}{200, 0, 0}

\lstset{style=mystyle}

\begin{document}

\maketitle

\textbf{Attention:} all code is available in

\url{https://github.com/Fernando-Urbano/causal-machine-learning/tree/main/hw4}.

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1}
  \section{Conditions on Nonparametric Estimators}
  
  We are studying the impact of a multi-valued treatment \( T \in \{0, 1, \dots, T\} \), for some integer \( T \), on an outcome \( Y \). We observe \( Z = (Y, T, \mathbf{X}')' \in \mathbb{R} \times \{0, 1, \dots, T\} \times \mathbb{R}^d \). Define the potential outcomes as \( Y(t) \), the propensity score \( p_t(\mathbf{x}) := \mathbb{P}[T = t \mid \mathbf{X} = \mathbf{x}] \), and the regression functions \( \mu_t(x) = \mathbb{E}[Y(t) \mid \mathbf{X} = \mathbf{x}] \).
  
  Interesting estimands can be built from averages of \( \mu_t(\mathbf{x}) \). For example: the ATE of treatment level \( t \) is \( \tau_t = \mathbb{E}[\mu_t(\mathbf{X}) - \mu_0(\mathbf{X})] \). If the treatment is a dose, then the effect of increasing the dose from \( t \) to \( t+1 \) is \( \mathbb{E}[\mu_{t+1}(\mathbf{X}) - \mu_t(\mathbf{X})] \). And so on. So we will study \( \mu_t = \mathbb{E}[\mu_t(\mathbf{X})] = \mathbb{E}[Y(t)] \).
  
  Suppose that \( p_t(x) = p_t \) is constant over \( x \), so that this is a randomized experiment.
  
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}
  
\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1a}
  \subsection{Linear Regression Model and Assumptions}
  
  Provide a single linear regression model that yields identification of all \( \mu_t \), \( t \in \{0, \dots, T\} \). What assumptions do you need? Describe the estimators \( \hat{\mu} \). Provide regularity conditions so that the vector \( \hat{\mu} \) is asymptotically Normal, asymptotically unbiased, and characterize the asymptotic variance.
  
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}
  
\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1b}
  \subsection{Sufficient Conditions for Identification}
  
  Now suppose that \( p_t(x) \) is not constant. Provide sufficient conditions so that \( \mu_t \) is identified. Compare these conditions to what you found above.
  
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}
  
\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  In class, we studied nonparametric regression using piecewise polynomials of degree \( p \) (fixed) and \( J = J_n \to \infty \) pieces and proved that the \( L_2 \) convergence rate is (using the notation of the present context)
  \[
  \|\hat{\mu}_t(\mathbf{x}) - \mu_t(\mathbf{x})\|_2^2 = O_p\left( \frac{J^d}{n} + J^{-2(p+1)} \right).
  \]

  Let the number of bins be \( J = C n^\gamma \) for some constants (positive) \( C \) and \( \gamma \). We will ignore \( C \) and focus on rates here.

  First we study nonparametric estimation and inference.

  \label{q1c}
  \subsection{Range of \(\gamma\) for Consistency}

  For what range of \( \gamma \) is \( \hat{\mu}_t(\mathbf{x}) \) consistent? How does this range depend on the dimension and the polynomial order? Are there values of \( p \) and \( d \) such that this interval is empty?
  
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}
  
\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1d}
  \subsection{Optimal Value of \(\gamma\)}
  
  What value of \( \gamma \) is optimal in the sense that the rate is the fastest? Call this \( \gamma^\star_{\text{mse}} \). How does \( \gamma^\star_{\text{mse}} \) vary with the dimension and the polynomial order?
  
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}
  
\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1e}
  \subsection{Asymptotic Normality of \(\hat{\mu}_t(x)\)}
  
  For what range of \( \gamma \) is \( \hat{\mu}_t(x) \) asymptotically Normal when properly centered and scaled? That is, determine the range for \( \gamma \) such that
  \[
  \sqrt{n / J^d} (\hat{\mu}_t(\mathbf{x}) - \mu_t(\mathbf{x})) \overset{d}{\to} \mathcal{N}(0, V).
  \]
  
  \rule{\textwidth}{0.5pt}

  (Don't worry about quantifying $V$). How does this range depend on the dimension and the polynomial order? Are there values of $p$ and $d$ such that this interval is empty?
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1f}
  \subsection{Range of \(\gamma\) for Optimal Rate \(\gamma^\star_{\text{mse}}\)}

  Is \( \gamma^\star_{\text{mse}} \) in this range?

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1g}
  \subsection{Semiparametric Estimation and Inference}

  In class, we showed that there was a problem with the two-step plug-in estimator \( \tilde{\mu}_t = \frac{1}{n} \sum_{i=1}^n \hat{\mu}(x_i) \) and that it did not have the same influence function as the parametric regression-based plug-in estimator. However, Cattaneo and Farrell (2011) showed that it does in fact obtain an influence function representation, with the familiar influence function. That paper shows that if
  \[
  \sqrt{n} \left( \frac{J^d}{n} + J^{-(p+1)} \right) \to 0
  \]
  then
  \[
  \sqrt{n} (\tilde{\mu}_t - \mathbb{E}[Y(t)]) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \psi_i + o_p(1) \overset{d}{\to} \mathcal{N}(0, \mathbb{E}[\psi_t(Z)^2]),
  \]
  where \( \psi_t(z_i) = \mu(x_i) - \mathbb{E}[Y(t)] + \mathbb{I}\{t_i = t\}(y_i - \mu(x_i)) / p_t(x_i) \).

  \begin{itemize}
      \item[(i)] For what range of \( \gamma \) is inference on the \( \mathbb{E}[Y(t)] \) possible? How does this range depend on \( p \) and \( d \)? Are there values of \( p \) and \( d \) such that this interval is empty?
      \item[(ii)] Is \( \gamma^\star_{\text{mse}} \) in this range?
  \end{itemize}

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q1h}
  \subsection{Influence Function-Based Estimator}

  Now consider the influence function-based estimator. Let \( \hat{\mu}_t(x) \) and \( \hat{p}_t(x) \) be partitioning-based estimators of the respective functions, which both have the rate of Equation (1). Define
  \[
  \hat{\mu}_t = \frac{1}{n} \sum_{i=1}^n \left\{ \hat{\mu}_t(x_i) + \frac{\mathbb{I}\{t_i = t\}(y_i - \hat{\mu}_t(x_i))}{\hat{p}_t(x_i)} \right\}.
  \]

  In class, we proved that the linear representation and asymptotic normality of Equation (3) holds (with \( \tilde{\mu}_t \) replaced by \( \hat{\mu}_t \)) if
  \[
  \|\hat{\mu}_t(x) - \mu_t(x)\|_2 \to 0, \quad \|\hat{p}_t(x) - p_t(x)\|_2 \to 0, \quad \text{and} \quad \sqrt{n} \|\hat{\mu}_t(x) - \mu_t(x)\|_2 \|\hat{p}_t(x) - p_t(x)\|_2 \to 0.
  \]

  \begin{itemize}
      \item[(i)] For what range of \( \gamma \) is inference on the \( \mathbb{E}[Y(t)] \) possible? How does this range depend on \( p \) and \( d \)? Are there values of \( p \) and \( d \) such that this interval is empty?
      \item[(ii)] Is \( \gamma^\star_{\text{mse}} \) in this range?
      \item[(iii)] In terms of the allowed \( \gamma \), compare your findings to the previous part.
  \end{itemize}

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q2}
  \section{Propensity Score Weighting \& ATT Estimation}

  \textit{This is a continuation from homeworks 2 \& 3.}

  Assume that the random variables \( (Y_1, Y_0, T, \mathbf{X}')' \in \mathbb{R} \times \mathbb{R} \times \{0, 1\} \times \mathbb{R}^d \) obey \( \{Y_1, Y_0\} \perp T \mid \mathbf{X} \). The researcher observes \( (Y, T, \mathbf{X}')' \), where \( Y = Y_1 T + Y_0 (1 - T) \). Define the propensity score \( p(x) = \mathbb{P}[T = 1 \mid \mathbf{X} = x] \) and assume it is bounded inside \( (0, 1) \). Define \( \mu_t = \mathbb{E}[Y(t) \mid T = 1] \) and \( \mu_t(x) = \mathbb{E}[Y(t) \mid \mathbf{X} = x] \). The average treatment effect on the treated (ATT) is \( \tau = \mu_1 - \mu_0 \).

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}

  In homework 3, you studied a “plug-in” estimator of the ATT given by
  \[
  \hat{\tau}_{\text{PI}} = \hat{\mu}_1 - \hat{\mu}_0 = \frac{1}{n} \sum_{i=1}^n \frac{t_i y_i}{\hat{p}} - \frac{1}{n} \sum_{i=1}^n \frac{(1 - t_i) \hat{p}(x_i) y_i}{(1 - \hat{p}(x_i))}.
  \]

  In homework 2, you proved that
  \[
  \mu_0 = \frac{1}{\mathbb{E}[T]} \mathbb{E}\left[ T \mu_0(X) + \frac{(1 - T) p(X) (Y - \mu_0(X))}{(1 - p(X))} \right]
  \]
  and that this moment condition is doubly robust. This motivates a doubly robust estimator of the ATT given by
  \[
  \hat{\tau}_{\text{DR}} = \hat{\mu}_1 - \hat{\mu}_0 = \frac{1}{n} \sum_{i=1}^n \left\{ \frac{t_i y_i}{\hat{p}} - \frac{1}{n} \sum_{i=1}^n \frac{(1 - t_i) \hat{p}(x_i) y_i}{(1 - \hat{p}(x_i))} \right\}.
  \]

  We will conduct a simulation study to examine various properties of these estimators. Make sure your simulation study obeys the data-generating process assumptions, including overlap. In this case, we know from theory that cross-fitting is not necessary, so we’ll skip it unless specifically asked for.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q2a}
  \subsection{High-Dimensional Parametric Case}

  (a) First, we study the high-dimensional parametric case. Suppose that \( \mu_0(x) = \beta_0' x \) and \( p(x) = (1 + \exp\{-\theta_0' x\})^{-1} \). Use a penalized linear model for \( \hat{\mu}_0(x_i) \) and a penalized logistic regression for \( \hat{p}(x) \). Try both LASSO and ridge regression.

  Find the sampling distribution of both estimators (4) and (5) as the data-generating process varies. In particular, try all combinations of the following:
  
  \begin{itemize}
      \item Sample size \( n = 1000 \) and \( 5000 \),
      \item Dimension \( d = \dim(x) = \{10, 50, 500, 5000\} \), and
      \item Sparsity levels \( s_\beta = \| \beta_0 \|_0 = \{d / 10, d / 2, d\} \) and \( s_0 = \| \theta_0 \|_0 = \{d / 10, d / 2, d\} \).
  \end{itemize}

  \begin{itemize}
      \item[(i)] What happens as \( n \) grows but \( d, s_\beta, s_0 \) are fixed?
      \item[(ii)] What happens to \( \hat{\tau}_{\text{PI}} \) as \( d \) and \( s_0 \) change for fixed \( n \)?
      \item[(iii)] How does \( s_\beta \) impact \( \hat{\tau}_{\text{PI}} \)?
      \item[(iv)] Verify the doubly robust property of \( \hat{\tau}_{\text{DR}} \).
      \item[(v)] What happens if you do not penalize in the first stage, but just use plain OLS and logistic regression?
      \item[(vi)] Discuss what your results mean for applied practice. When would you recommend the different estimators and why?
  \end{itemize}
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}  
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q2b}
  \subsection{Nonparametrics and Low-Dimensional Case}

  (b) Now we turn to nonparametrics and lower-dimensional functions. Suppose that \( \mu_0(x) \) and \( p(x) \) are completely unknown functions. In your data-generating process, make them nonlinear functions of \( x^2 \). Try \( n = \{1000, 5000, 15000\} \) and \( d = \dim(x) = \{1, 3, 5, 10\} \), including designs with sparsity. Use deep nets and random forests (and anything else you care to try).

  \begin{itemize}
    \item[(i)] What happens as \( n \) grows but \( d \) is fixed?
    \item[(ii)] Verify the doubly robus property of $\hat{\tau}_{\text{DR}}$.
    \item[(iii)] Dicuss what your results mena for applied practice. When would you recommend the different estimators and why?
    \item[(iv)] Verify the doubly robust property of \( \hat{\tau}_{\text{DR}} \).
    \item[(v)] What happens if you do not penalize in the first stage, but just use plain OLS and logistic regression?
    \item[(vi)] Discuss what your results mean for applied practice. When would you recommend the different estimators and why?
\end{itemize}

  Now real data. Return to the Census data from class to find the ATT of sex on the log wage rate.
  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}  
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q2c}
  \subsection{Discuss Results}
  (c) Show results:
  \begin{itemize}
      \item[(i)] Both estimators (4) and (5),
      \item[(ii)] With and without cross-fitting,
      \item[(iii)] Using different first-step estimators for the propensity score \( \hat{p}(x_i) \) and regression function \( \hat{\mu}_0(x_i) \), including forests, neural networks, LASSO, and parametric models.
  \end{itemize}

  Discuss the results.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3}
  \section{An Application}

  The file \texttt{data\_for\_HW4.csv} contains data from two independent sources, as indicated by the variable \( e \). Both have data on the same outcome \( y \), same treatment \( t \), and the same set of pre-treatment variables \( x.1, x.2, x.3, x.4, x.5 \). The treatment in the first data source may have been targeted based on some or all of the \( x \) variables. The second data source is a fully randomized experiment. Both obey our other assumptions (SUTVA, consistency, CIA, overlap).

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3a}
  \subsection{Ignoring \( x \) Variables}

  Ignore the \( x \) variables to compute the ATE and a confidence interval for it in each of the data sources. Comment on your findings and possible explanations for them.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3b}
  \subsection{Linear Model with Interactions}

  Use a linear model with interactions to obtain the CATEs in each data source, plot the distribution of the CATEs, obtain the ATE and its confidence interval. Compare your findings on the ATEs to the previous part.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3c}
  \subsection{Doubly Robust Estimation}

  Combine the estimators of \( \mu_t(x) = \mathbb{E}[Y(t) \mid X = x] \) with a parametric logistic regression estimate of the propensity score \( p(x) = \mathbb{P}[T = 1 \mid X = x] \) to estimate the ATE and confidence interval in each data source using the doubly robust estimator. Compare your findings on the ATEs to the previous two parts.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3d}
  \subsection{Flexible/Nonparametric Versions}

  Replace your estimates of \( \mu_t(x) \) and \( p(x) \) with flexible/nonparametric/ML versions, and repeat the doubly robust estimation and inference. Try a few different nonparametric estimators for practice.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\begin{figure}[H]
  \begin{colorparagraph}{questioncolor}
  \rule{\textwidth}{0.5pt}
  \label{q3e}
  \subsection{Combined Model for Both Datasets}

  Propose and estimate a model (parametric or not) that combines and uses the two datasets as one. In other words, your model should have a single loss function, shared or common parameters, and appropriate assumptions as you deem fit. You must use data from both sources. Discuss your choice of specification and the properties of your proposed estimator.

  \rule{\textwidth}{0.5pt}
  \end{colorparagraph}
\end{figure}

\end{document}
